{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aec5096c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta import *\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer\n",
    "from dataclasses import dataclass\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry import Polygon\n",
    "from pyspark.sql import DataFrame\n",
    "from os import getcwd\n",
    "import shutil\n",
    "import os\n",
    "from sedona.sql.types import GeometryType\n",
    "from shapely.wkt import loads\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19003304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup():\n",
    "    if os.path.exists(\"./spark-warehouse\"):\n",
    "        shutil.rmtree(\"./spark-warehouse\")\n",
    "cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a92f188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_path(table_name: str) ->str :\n",
    "    return f\"{getcwd()}/spark-warehouse/{table_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b480412f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GeomExtent:\n",
    "    geom: Polygon\n",
    "    identifier: int\n",
    "\n",
    "        \n",
    "@dataclass\n",
    "class CityMeasure:\n",
    "    city: str\n",
    "    index: int\n",
    "    measure: float\n",
    "    geom: str\n",
    "        \n",
    "        \n",
    "def create_table_as_select(delta_table_name: str, data_frame: DataFrame):\n",
    "    view_name = f\"{delta_table_name}_view\"\n",
    "    data_frame.createOrReplaceTempView(view_name)\n",
    "    spark.sql(\n",
    "        f\"\"\"\n",
    "            CREATE TABLE {delta_table_name} using delta LOCATION '{get_table_path(delta_table_name)}' AS SELECT * FROM {view_name}\n",
    "        \"\"\"\n",
    "    ).show()\n",
    "    \n",
    "    \n",
    "def load_delta_table(table_name: str) -> DataFrame:\n",
    "    return spark.sql(f\"SELECT * FROM delta.`{get_table_path(table_name)}`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d639a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/01 12:20:11 WARN Utils: Your hostname, macpro.local resolves to a loopback address: 127.0.0.1; using 192.168.0.100 instead (on interface en0)\n",
      "22/10/01 12:20:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/Users/pawel.kocinski/Library/Caches/pypoetry/virtualenvs/binder-7uni60N0-py3.8/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/pawel.kocinski/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/pawel.kocinski/.ivy2/jars\n",
      "org.apache.sedona#sedona-python-adapter-3.0_2.12 added as a dependency\n",
      "org.datasyslab#geotools-wrapper added as a dependency\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-180758d7-bd3a-4285-833d-431142bd21e0;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.sedona#sedona-python-adapter-3.0_2.12;1.2.1-incubating in central\n",
      "\tfound org.locationtech.jts#jts-core;1.18.2 in local-m2-cache\n",
      "\tfound org.wololo#jts2geojson;0.16.1 in central\n",
      "\tfound org.apache.sedona#sedona-core-3.0_2.12;1.2.1-incubating in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.5.0 in local-m2-cache\n",
      "\tfound org.apache.sedona#sedona-sql-3.0_2.12;1.2.1-incubating in central\n",
      "\tfound org.datasyslab#geotools-wrapper;1.1.0-25.2 in central\n",
      "\tfound io.delta#delta-core_2.12;2.1.0 in central\n",
      "\tfound io.delta#delta-storage;2.1.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in local-m2-cache\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in local-m2-cache\n",
      ":: resolution report :: resolve 245ms :: artifacts dl 13ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.1.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.1.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from local-m2-cache in [default]\n",
      "\torg.apache.sedona#sedona-core-3.0_2.12;1.2.1-incubating from central in [default]\n",
      "\torg.apache.sedona#sedona-python-adapter-3.0_2.12;1.2.1-incubating from central in [default]\n",
      "\torg.apache.sedona#sedona-sql-3.0_2.12;1.2.1-incubating from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from local-m2-cache in [default]\n",
      "\torg.datasyslab#geotools-wrapper;1.1.0-25.2 from central in [default]\n",
      "\torg.locationtech.jts#jts-core;1.18.2 from local-m2-cache in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.5.0 from local-m2-cache in [default]\n",
      "\torg.wololo#jts2geojson;0.16.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-180758d7-bd3a-4285-833d-431142bd21e0\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 11 already retrieved (0kB/6ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/01 12:20:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"MyApp\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.serializer\", KryoSerializer.getName) \\\n",
    "    .config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName) \\\n",
    "    .config('spark.jars.packages',\n",
    "           'org.apache.sedona:sedona-python-adapter-3.0_2.12:1.2.1-incubating,org.datasyslab:geotools-wrapper:1.1.0-25.2,io.delta:delta-core_2.12:2.1.0') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e98c1810",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SedonaRegistrator.registerAll(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401bbee6",
   "metadata": {},
   "source": [
    "# Create Delta table with geometry type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cb3fd1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ceb064f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<delta.tables.DeltaTable at 0x7fe6dad34460>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DeltaTable.\\\n",
    "    create(spark).\\\n",
    "    location(\"locations\").\\\n",
    "    addColumn(\"identifier\", \"BIGINT\").\\\n",
    "    addColumn(\"geom\", GeometryType()).\\\n",
    "    execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e36ca1",
   "metadata": {},
   "source": [
    "# Create delta table as select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0134482f",
   "metadata": {},
   "outputs": [],
   "source": [
    "geospatial_df = spark.createDataFrame(\n",
    "    [\n",
    "        CityMeasure(index = 1, geom=\"POINT(19.936160 50.061486)\", city=\"Cracow\", measure = 0.0),\n",
    "        CityMeasure(index = 1, geom=\"POINT(21.037308 52.226236)\", city=\"Warsaw\", measure = 0.0),\n",
    "        CityMeasure(index = 1, geom=\"POINT(4.899059 52.363573)\", city=\"Amsterdam\", measure = 0.0),\n",
    "        CityMeasure(index = 1, geom=\"POINT(-118.240286 34.004080)\", city=\"Los Angeles\", measure = 0.0),\n",
    "        CityMeasure(index = 1, geom=\"POINT(-77.054092 38.934715)\", city=\"Washington\", measure = 0.0),\n",
    "        CityMeasure(index = 1, geom=\"POINT(2.152872 41.377910)\", city=\"Barcelona\", measure = 0.0),\n",
    "        CityMeasure(index = 1, geom=\"POINT(13.419083 52.504049)\", city=\"Berlin\", measure = 0.0),\n",
    "        CityMeasure(index = 1, geom=\"POINT(-74.047214 40.742164)\", city=\"New York\", measure = 0.0)\n",
    "        \n",
    "    ],\n",
    ")\n",
    "geospatial_df.createOrReplaceTempView(\"sedona_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92b4fe04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+-----+-------+\n",
      "|       city|                geom|index|measure|\n",
      "+-----------+--------------------+-----+-------+\n",
      "|     Cracow|POINT(19.936160 5...|    1|    0.0|\n",
      "|     Warsaw|POINT(21.037308 5...|    1|    0.0|\n",
      "|  Amsterdam|POINT(4.899059 52...|    1|    0.0|\n",
      "|Los Angeles|POINT(-118.240286...|    1|    0.0|\n",
      "| Washington|POINT(-77.054092 ...|    1|    0.0|\n",
      "|  Barcelona|POINT(2.152872 41...|    1|    0.0|\n",
      "|     Berlin|POINT(13.419083 5...|    1|    0.0|\n",
      "|   New York|POINT(-74.047214 ...|    1|    0.0|\n",
      "+-----------+--------------------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "geospatial_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "415290df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/01 12:20:22 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "22/10/01 12:20:22 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "22/10/01 12:20:22 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 12:===================>                                     (4 + 8) / 12]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_name = \"sedona_delta\"\n",
    "\n",
    "spark.sql(\n",
    "f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {table_name} using delta LOCATION '{get_table_path(table_name)}' AS SELECT * FROM sedona_table\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f5febb",
   "metadata": {},
   "source": [
    "# Insert into table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02bb5215",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_table_name = \"insert_geospatial_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9e98d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "europe_extent = \"\"\"\n",
    "POLYGON((\n",
    "    -21.289062499999996 73.22863503912812,\n",
    "    43.046875 73.22863503912812,\n",
    "    43.046875 38.82781590516771,\n",
    "    -21.289062499999996 38.82781590516771,\n",
    "    -21.289062499999996 73.22863503912812\n",
    "))\n",
    "\"\"\".replace(\"  \", \"\").replace(\"\\n\", \"\")\n",
    "\n",
    "\n",
    "\n",
    "europe_extent = spark.createDataFrame(\n",
    "    [GeomExtent(loads(europe_extent), 1)]\n",
    ")\n",
    "europe_extent.createOrReplaceTempView(\"extent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13424fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "existing_records = spark.createDataFrame(\n",
    "    [\n",
    "        CityMeasure(index = 6, geom=loads(\"POINT(-77.054092 38.934715)\"), city=\"Washington\", measure = 0.0),\n",
    "    ],\n",
    ")\n",
    "\n",
    "create_table_as_select(delta_table_name, existing_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d9c7954",
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_records = spark.createDataFrame(\n",
    "    [\n",
    "        CityMeasure(index = 1, geom=loads(\"POINT(21.037308 52.226236)\"), city=\"Warsaw\", measure = 0.0),\n",
    "        CityMeasure(index = 2, geom=loads(\"POINT(4.899059 52.363573)\"), city=\"Amsterdam\", measure = 0.0),\n",
    "        CityMeasure(index = 6, geom=loads(\"POINT(-77.054092 38.934715)\"), city=\"Washington\", measure = 0.0),\n",
    "        CityMeasure(index = 3, geom=loads(\"POINT(13.419083 52.504049)\"), city=\"Berlin\", measure = 0.0),\n",
    "    ],\n",
    ")\n",
    "\n",
    "geospatial_data_view_name = \"new_records\"\n",
    "additional_records.createOrReplaceTempView(geospatial_data_view_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "522fdb85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----+-------+\n",
      "|      city|                geom|index|measure|\n",
      "+----------+--------------------+-----+-------+\n",
      "|Washington|POINT (-77.054092...|    6|    0.0|\n",
      "+----------+--------------------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "load_delta_table(delta_table_name).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ffa86df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/01 12:20:25 WARN RangeJoinExec: [SedonaSQL] Join dominant side partition number 12 is larger than 1/2 of the dominant side count 4\n",
      "22/10/01 12:20:25 WARN RangeJoinExec: [SedonaSQL] Try to use follower side partition number 12\n",
      "22/10/01 12:20:25 WARN RangeJoinExec: [SedonaSQL] Join follower side partition number is also larger than 1/2 of the dominant side count 4\n",
      "22/10/01 12:20:25 WARN RangeJoinExec: [SedonaSQL] Try to use 1/2 of the dominant side count 2 as the partition number of both sides\n",
      "22/10/01 12:20:25 WARN JoinQuery: UseIndex is true, but no index exists. Will build index on the fly.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\n",
    "f\"\"\"\n",
    "INSERT INTO delta.`{get_table_path(delta_table_name)}` (city, geom, index, measure)\n",
    "    SELECT i.city, i.geom, i.index, i.measure FROM {geospatial_data_view_name} AS i, extent AS b\n",
    "    WHERE St_Intersects(i.geom, b.geom)\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12e3c471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----+-------+\n",
      "|      city|                geom|index|measure|\n",
      "+----------+--------------------+-----+-------+\n",
      "| Amsterdam|POINT (4.899059 5...|    2|    0.0|\n",
      "|    Berlin|POINT (13.419083 ...|    3|    0.0|\n",
      "|Washington|POINT (-77.054092...|    6|    0.0|\n",
      "|    Warsaw|POINT (21.037308 ...|    1|    0.0|\n",
      "+----------+--------------------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "load_delta_table(delta_table_name).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71100074",
   "metadata": {},
   "source": [
    "# Update Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b177029f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    " UPDATE delta.`{get_table_path(delta_table_name)}`\n",
    " SET measure = 1.0 where ST_Distance(geom, ST_GeomFromText('POINT(21.00 52.00)')) < 1.0\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20976dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----+-------+\n",
      "|      city|                geom|index|measure|\n",
      "+----------+--------------------+-----+-------+\n",
      "| Amsterdam|POINT (4.899059 5...|    2|    0.0|\n",
      "|    Berlin|POINT (13.419083 ...|    3|    0.0|\n",
      "|Washington|POINT (-77.054092...|    6|    0.0|\n",
      "|    Warsaw|POINT (21.037308 ...|    1|    1.0|\n",
      "+----------+--------------------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "load_delta_table(delta_table_name).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e9d02e",
   "metadata": {},
   "source": [
    "# Merge into table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68937f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_records_to_merge = spark.createDataFrame(\n",
    "    [\n",
    "        CityMeasure(index = 1, geom=loads(\"POINT(21.037308 52.226236)\"), city=\"Warsaw\", measure = 2.0),\n",
    "        CityMeasure(index = 5, geom=loads(\"POINT(4.899059 52.363573)\"), city=\"Amsterdam\", measure = 4.0),\n",
    "        CityMeasure(index = 6, geom=loads(\"POINT(-77.054092 38.934715)\"), city=\"Washington\", measure = 5.0),\n",
    "    ]\n",
    ").repartition(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d3d42c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "geospatial_data_to_merge_view_name = \"new_records\"\n",
    "additional_records_to_merge.createOrReplaceTempView(geospatial_data_view_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13441f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/01 12:20:28 WARN RangeJoinExec: [SedonaSQL] Join dominant side partition number 5 is larger than 1/2 of the dominant side count 4\n",
      "22/10/01 12:20:28 WARN RangeJoinExec: [SedonaSQL] Try to use follower side partition number 1\n",
      "22/10/01 12:20:28 WARN JoinQuery: UseIndex is true, but no index exists. Will build index on the fly.\n",
      "22/10/01 12:20:29 WARN MergeIntoCommand: Merge source has SQLMetric(id: 1586, name: Some(number of source rows), value: 3) rows in initial scan but SQLMetric(id: 1587, name: Some(number of source rows (during repeated scan)), value: 6) rows in second scan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint, num_updated_rows: bigint, num_deleted_rows: bigint, num_inserted_rows: bigint]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    " MERGE INTO delta.`{get_table_path(delta_table_name)}` AS m\n",
    " USING {geospatial_data_to_merge_view_name} AS u\n",
    " ON ST_Intersects(st_buffer(m.geom, 2.0), u.geom)\n",
    " WHEN MATCHED THEN\n",
    " UPDATE SET\n",
    "  measure = u.measure\n",
    " WHEN NOT MATCHED\n",
    " THEN INSERT (\n",
    " index,\n",
    " geom,\n",
    " measure,\n",
    " city\n",
    " )\n",
    " VALUES(\n",
    "  u.index,\n",
    "  u.geom,\n",
    "  u.measure,\n",
    "  u.city\n",
    " )\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "805a4fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----+-------+\n",
      "|      city|                geom|index|measure|\n",
      "+----------+--------------------+-----+-------+\n",
      "|    Warsaw|POINT (21.037308 ...|    1|    2.0|\n",
      "| Amsterdam|POINT (4.899059 5...|    2|    4.0|\n",
      "|Washington|POINT (-77.054092...|    6|    5.0|\n",
      "|    Berlin|POINT (13.419083 ...|    3|    0.0|\n",
      "+----------+--------------------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "load_delta_table(delta_table_name).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1b5230",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
